{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# from preprocess_c import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "train_path  = '../data/protechn_corpus_eval/train'\n",
    "test_path = '../data/protechn_corpus_eval/test'\n",
    "dev_path = '../data/protechn_corpus_eval/dev'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_dset(path):\n",
    "#     path_ = Path(path)\n",
    "#     a = make_dataset(path_)\n",
    "#     df_1 = pd.DataFrame(columns=['id','full_sent','start_sent','end_sent','start_prop','end_prop','prop','??','???'])\n",
    "#     for dm in a:\n",
    "#         df_t = pd.DataFrame(dm,columns =['id','full_sent','start_sent','end_sent','start_prop','end_prop','prop','??','???'] )\n",
    "#         df_1 = df_1.append(df_t,ignore_index= True)\n",
    "#     return df_1.iloc[:,:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train = make_dset(train_path)\n",
    "df_train= pd.read_csv('ADD5_Context5.csv')\n",
    "df_test = pd.read_csv('tuan_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test[df_test['full_sent'].str.len() > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonme = [0 if i == 'O' else 1 for i in df_train.prop.values ]\n",
    "df_train['binary'] = lonme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bi = [0 if i == 'O' else 1 for i in df_test.prop.values ]\n",
    "df_test['binary'] = test_bi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mapping = {'Loaded_Language':1,'Name_Calling,Labeling':2,'Repetition':3,\n",
    "           'Exaggeration,Minimisation':4,'Doubt':5,'Appeal_to_fear-prejudice':6,'Flag-Waving':7,'Causal_Oversimplification':8,\n",
    "           'Slogans':9,'Appeal_to_Authority':10,'Black-and-White_Fallacy':11,'Thought-terminating_Cliches':12,'Whataboutism':13,\n",
    "           'Reductio_ad_hitlerum':14,'Red_Herring':15,'Bandwagon':16,'Obfuscation,Intentional_Vagueness,Confusion':17,'Straw_Men':18,'O':0}\n",
    "#df_train = df_train[df_train.binary !=0]\n",
    "\n",
    "df_train['prop_1'] = df_train.prop.apply(lambda x: mapping[x])\n",
    "df_test['prop_1'] = df_test.prop.apply(lambda x: mapping[x])\n",
    "# df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_nm = df_test[df_test['prop'] != \"O\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "# train_direct = glob.glob('data/data_propoganda/data/protechn_corpus_eval/train/*.txt')\n",
    "# articles = []\n",
    "# def read_articles():\n",
    "#   for filename in train_direct:\n",
    "#     myfile = open(filename)\n",
    "#     article = myfile.read()\n",
    "#     articles.append(article)\n",
    "#     myfile.close()\n",
    "#   article_ids = []\n",
    "#   for filename in train_direct:\n",
    "#     article_ids.append(filename[60:-4])\n",
    "  \n",
    "#   return articles, article_ids\n",
    "# articles,art_ids = read_articles()\n",
    "# id2art ={i:a for a,i in zip(articles,art_ids)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_context(article, span, mode='sentence'):\n",
    "#   article = id2art[article]\n",
    "#   def get_num_words(sentence):\n",
    "#     return len(sentence.split(' '))\n",
    "#   if mode == \"title\":\n",
    "#     return article.split('\\n')[0]\n",
    "#   if mode == \"sentence\":\n",
    "#     WORD_LEN_LIMIT = 120\n",
    "#     li = span[0]\n",
    "#     ri = span[1]\n",
    "#     span_text = article[li: ri]\n",
    "#     num_words = get_num_words(span_text)\n",
    "#     if num_words >= WORD_LEN_LIMIT:\n",
    "#       return span_text\n",
    "#     remaining_len = WORD_LEN_LIMIT - num_words\n",
    "#     lhs_words = remaining_len // 2\n",
    "#     rhs_words = remaining_len - lhs_words\n",
    "#     li -= 1\n",
    "#     lcount = 0\n",
    "#     while li >= 0 and article[li-1] != '\\n' and lcount < lhs_words:\n",
    "#       if article[li] == ' ':\n",
    "#         lcount += 1\n",
    "#       li -= 1\n",
    "#     ri += 1\n",
    "#     rcount = 0\n",
    "#     while ri < len(article) and article[ri] != '\\n' and rcount < rhs_words:\n",
    "#       if article[ri] == ' ':\n",
    "#         rcount += 1\n",
    "#       ri += 1\n",
    "#     return article[li+1: ri - 1] \n",
    "\n",
    "#   return \"\"\n",
    "# spans_1 = [(i,k,j) for i,k,j in zip(df_train.id,df_train.start_sent,df_train.end_sent)]\n",
    "# df_train['context'] = [get_context(i,(s,e)) for i,s,e in spans_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the device for GPU usage\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Triage(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.data.full_sent[index])\n",
    "        title = \" \".join(title.split())\n",
    "        context = str(self.data.context[index])\n",
    "        context = \" \".join(context.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        tokenized_context = self.tokenizer.encode_plus(context,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=self.max_len,\n",
    "                                            pad_to_max_length=True,\n",
    "                                            return_attention_mask=True,truncation = True)\n",
    "\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        c_ids = tokenized_context['input_ids']\n",
    "        c_mask = tokenized_context['attention_mask']\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.prop_1[index], dtype=torch.long),\n",
    "            'c_ids':torch.tensor(c_ids, dtype=torch.long),\n",
    "            'c_mask': torch.tensor(c_mask,dtype= torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 16\n",
    "# VALID_RARE_BATCH_SIZE = int(df_test_nm.__len__()/4)\n",
    "\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (42948, 10)\n",
      "TRAIN Dataset: (42948, 10)\n",
      "TEST Dataset: (4490, 10)\n",
      "TEST Non-majourity Dataset: (1490, 10)\n"
     ]
    }
   ],
   "source": [
    "train_size = 1\n",
    "train_dataset=df_train.sample(frac=train_size,random_state=200)\n",
    "# test_dataset=df_train.drop(train_dataset.index).reset_index(drop=True)\n",
    "test_dataset = df_test.reset_index(drop=True)\n",
    "test_nm_dataset = df_test_nm.reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df_train.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "print(\"TEST Non-majourity Dataset: {}\".format(test_nm_dataset.shape))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "training_set = Triage(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = Triage(test_dataset, tokenizer, MAX_LEN)\n",
    "testing_nm_set = Triage(test_nm_dataset, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0,\n",
    "                'drop_last': False\n",
    "                }\n",
    "\n",
    "\n",
    "# test_nm_params = {'batch_size': VALID_RARE_BATCH_SIZE,\n",
    "#                 'shuffle': False,\n",
    "#                 'num_workers': 0,\n",
    "#                 'drop_last': False\n",
    "#                 }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)\n",
    "# testing_nm_loader = DataLoader(testing_nm_set, **test_nm_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss, MSELoss,BCEWithLogitsLoss\n",
    "\n",
    "class ContextualBertForSequenceClassification(torch.nn.Module):\n",
    "  \n",
    "  def __init__(self, num_labels, ContextModel, SpanModel):\n",
    "    super(ContextualBertForSequenceClassification, self).__init__()\n",
    "    self.ContextModel = ContextModel\n",
    "    self.SpanModel = SpanModel\n",
    "    self.num_labels = num_labels\n",
    "\n",
    "    # self.classifier = torch.nn.Linear(768*2, num_labels)\n",
    "    # self.classifier1 = torch.nn.Linear(768, num_labels)\n",
    "    self.classifier2 = torch.nn.Linear(768+128, num_labels)\n",
    "    self.reduce_classifier = torch.nn.Linear(768, 128)\n",
    "    self.dropout = torch.nn.Dropout(0.1)\n",
    "\n",
    "  def forward(\n",
    "      self,\n",
    "      span_input_ids,\n",
    "      span_attention_mask,\n",
    "      context_input_ids,\n",
    "      context_attention_mask,\n",
    "      labels=None\n",
    "  ):\n",
    "    context_outputs = self.ContextModel(\n",
    "        input_ids=context_input_ids,\n",
    "        attention_mask=context_attention_mask\n",
    "    )\n",
    "    context_outputs = context_outputs[1] # pooler output\n",
    "    span_outputs = self.SpanModel(\n",
    "        input_ids=span_input_ids,\n",
    "        attention_mask=span_attention_mask\n",
    "    )\n",
    "    span_outputs = span_outputs[1]\n",
    "\n",
    "    context_outputs = self.reduce_classifier(context_outputs)\n",
    "    pooled_output = torch.cat((span_outputs, context_outputs), axis=1)\n",
    "\n",
    "    pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "    logits = self.classifier2(pooled_output)\n",
    "    outputs = (logits,)\n",
    "    if labels is not None:\n",
    "      if self.num_labels == 1:\n",
    "        loss_fct = MSELoss()\n",
    "        loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "      else:\n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "      outputs = (loss,) + outputs\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class DistillBERTBinary(torch.nn.Module):\\n    def __init__(self):\\n        super(DistillBERTBinary, self).__init__()\\n        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\\n        self.pre_classifier = torch.nn.Linear(768, 768)\\n        self.dropout = torch.nn.Dropout(0.3)\\n        self.classifier = torch.nn.Linear(768, 1)\\n        self.softmax = torch.nn.Sigmoid()\\n\\n    def forward(self, input_ids, attention_mask):\\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\\n        hidden_state = output_1[0]\\n        pooler = hidden_state[:, 0]\\n        pooler = self.pre_classifier(pooler)\\n        pooler = torch.nn.ReLU()(pooler)\\n        pooler = self.dropout(pooler)\\n        output =self.classifier(pooler)\\n        return output'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"class DistillBERTBinary(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTBinary, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 1)\n",
    "        self.softmax = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output =self.classifier(pooler)\n",
    "        return output\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \\n\\nclass DistillBERTClass(torch.nn.Module):\\n    def __init__(self):\\n        super(DistillBERTClass, self).__init__()\\n        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\\n        self.pre_classifier = torch.nn.Linear(768, 768)\\n        self.dropout = torch.nn.Dropout(0.3)\\n        self.classifier = torch.nn.Linear(768, 18)\\n        self.softmax = torch.nn.Softmax(19)\\n\\n    def forward(self, input_ids, attention_mask):\\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\\n        hidden_state = output_1[0]\\n        pooler = hidden_state[:, 0]\\n        pooler = self.pre_classifier(pooler)\\n        pooler = torch.nn.ReLU()(pooler)\\n        pooler = self.dropout(pooler)\\n        output = self.classifier(pooler)\\n        return output'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class DistillBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistillBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.classifier = torch.nn.Linear(768, 18)\n",
    "        self.softmax = torch.nn.Softmax(19)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        return output\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "  pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "  labels_flat = labels.flatten()\n",
    "  return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs=5):\n",
    "  loss_values = []\n",
    "  for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for step, batch in enumerate(training_loader):\n",
    "      if step % 100 == 0 and not step == 0:\n",
    "        elapsed = format_time(time.time() - t0)\n",
    "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(training_loader), elapsed))\n",
    "      b_input_ids = batch['ids'].to(device)\n",
    "      b_labels = batch['targets'].to(device, dtype = torch.long)\n",
    "      b_input_mask = batch['mask'].to(device)\n",
    "      b_c_input_ids = batch['c_ids'].to(device)\n",
    "      b_c_input_mask = batch['c_mask'].to(device)\n",
    "      model.zero_grad()        \n",
    "      outputs = model(b_input_ids, \n",
    "                      b_input_mask,\n",
    "                      b_c_input_ids, \n",
    "                      b_c_input_mask, \n",
    "                      labels=b_labels)\n",
    "      loss = outputs[0]\n",
    "      total_loss += loss.item()\n",
    "      loss.backward()\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "      optimizer.step()\n",
    "      scheduler.step() # TODO\n",
    "      stats = '[%d/%d][%d/%d]\\tLoss: %.4f' \\\n",
    "                % (epoch_i+1, epochs, step, len(training_loader), loss.item())\n",
    "      print('\\r' + stats, end=\"\")\n",
    "      sys.stdout.flush()\n",
    "    avg_train_loss = total_loss / len(training_loader)            \n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in testing_loader:\n",
    "      # batch = tuple(t.to(device) for t in batch)\n",
    "      b_input_ids = batch['ids'].to(device)\n",
    "      b_labels = batch['targets'].to(device, dtype = torch.long)\n",
    "      b_input_mask = batch['mask'].to(device)\n",
    "      b_c_input_ids = batch['c_ids'].to(device)\n",
    "      b_c_input_mask = batch['c_mask'].to(device)\n",
    "      with torch.no_grad():        \n",
    "        outputs = model(b_input_ids, \n",
    "                        b_input_mask,\n",
    "                        b_c_input_ids, \n",
    "                        b_c_input_mask)\n",
    "      logits = outputs[0]\n",
    "      logits = logits.detach().cpu().numpy()\n",
    "      label_ids = b_labels.to('cpu').numpy()\n",
    "      tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "      eval_accuracy += tmp_eval_accuracy\n",
    "      nb_eval_steps += 1\n",
    "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
    "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
    "  print(\"\")\n",
    "  print(\"Training complete!\")\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(model, dataloader):\n",
    "  model.eval()\n",
    "  predictions , true_labels = [], []\n",
    "  nb_eval_steps = 0\n",
    "  for batch in dataloader:\n",
    "    b_input_ids = batch['ids'].to(device)\n",
    "    b_labels = batch['targets'].to(device)\n",
    "    b_input_mask = batch['mask'].to(device)\n",
    "    b_c_input_ids = batch['c_ids'].to(device)\n",
    "    b_c_input_mask = batch['c_mask'].to(device)\n",
    "    with torch.no_grad():        \n",
    "      logits = model(b_input_ids, \n",
    "                     b_input_mask,\n",
    "                     b_c_input_ids, \n",
    "                     b_c_input_mask)\n",
    "    logits = logits[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    pred_label = np.argmax(logits, axis=1) #[1 if a >0.5 else 0 for a in logits]\n",
    "    predictions.extend(pred_label)\n",
    "    true_labels.extend(label_ids)\n",
    "  return predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dev_predictions(model):\n",
    "  test_articles, _ = read_articles(\"dev-articles\")\n",
    "  test_spans, test_techniques = read_test_spans()\n",
    "\n",
    "  test_articles = test_articles[1:]\n",
    "  test_dataloader = get_data(test_articles, test_spans, test_techniques)\n",
    "  pred, _ = get_model_predictions(model, test_dataloader)\n",
    "\n",
    "  with open('predictions.txt', 'w') as fp:\n",
    "    label_file = os.path.join(data_dir, \"dev-task-TC-template.out\")\n",
    "    myfile = open(label_file)\n",
    "    prev_index = -1\n",
    "    tsvreader = csv.reader(myfile, delimiter=\"\\t\")\n",
    "    for i, row in enumerate(tsvreader):\n",
    "      fp.write(row[0] + '\\t' + distinct_techniques[pred[i]] + '\\t' + row[2] + '\\t' + row[3] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextualBertForSequenceClassification(\n",
       "  (ContextModel): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (SpanModel): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier2): Linear(in_features=896, out_features=19, bias=True)\n",
       "  (reduce_classifier): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "#from transformers import RobertaModel\n",
    "from transformers import BertModel\n",
    "#from transformers import RobertaForSequenceClassification\n",
    "import time,sys\n",
    "\n",
    "model_name = 'bert-base-cased'\n",
    "context_model = BertModel.from_pretrained(model_name)\n",
    "span_model = BertModel.from_pretrained(model_name)\n",
    "model = ContextualBertForSequenceClassification(19, context_model, span_model)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 7 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/m/home/home2/26/nguyenl21/unix/DSP/venv/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1773: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/7][99/1343]\tLoss: 2.1379  Batch   100  of  1,343.    Elapsed: 0:01:40.\n",
      "[1/7][199/1343]\tLoss: 1.5695  Batch   200  of  1,343.    Elapsed: 0:03:22.\n",
      "[1/7][299/1343]\tLoss: 1.8469  Batch   300  of  1,343.    Elapsed: 0:05:05.\n",
      "[1/7][399/1343]\tLoss: 1.0334  Batch   400  of  1,343.    Elapsed: 0:06:48.\n",
      "[1/7][499/1343]\tLoss: 1.2125  Batch   500  of  1,343.    Elapsed: 0:08:30.\n",
      "[1/7][599/1343]\tLoss: 1.3641  Batch   600  of  1,343.    Elapsed: 0:10:12.\n",
      "[1/7][699/1343]\tLoss: 0.8691  Batch   700  of  1,343.    Elapsed: 0:11:55.\n",
      "[1/7][799/1343]\tLoss: 0.9900  Batch   800  of  1,343.    Elapsed: 0:13:37.\n",
      "[1/7][899/1343]\tLoss: 1.3946  Batch   900  of  1,343.    Elapsed: 0:15:19.\n",
      "[1/7][999/1343]\tLoss: 1.1283  Batch 1,000  of  1,343.    Elapsed: 0:17:01.\n",
      "[1/7][1099/1343]\tLoss: 1.0795  Batch 1,100  of  1,343.    Elapsed: 0:18:43.\n",
      "[1/7][1199/1343]\tLoss: 1.0621  Batch 1,200  of  1,343.    Elapsed: 0:20:26.\n",
      "[1/7][1299/1343]\tLoss: 0.7990  Batch 1,300  of  1,343.    Elapsed: 0:22:08.\n",
      "[1/7][1342/1343]\tLoss: 1.3329\n",
      "  Average training loss: 1.35\n",
      "  Training epcoh took: 0:22:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.68\n",
      "  Validation took: 0:00:53\n",
      "\n",
      "======== Epoch 2 / 7 ========\n",
      "Training...\n",
      "[2/7][99/1343]\tLoss: 0.7604  Batch   100  of  1,343.    Elapsed: 0:01:42.\n",
      "[2/7][199/1343]\tLoss: 0.8131  Batch   200  of  1,343.    Elapsed: 0:03:24.\n",
      "[2/7][299/1343]\tLoss: 0.6786  Batch   300  of  1,343.    Elapsed: 0:05:06.\n",
      "[2/7][399/1343]\tLoss: 0.6699  Batch   400  of  1,343.    Elapsed: 0:06:48.\n",
      "[2/7][499/1343]\tLoss: 0.5827  Batch   500  of  1,343.    Elapsed: 0:08:31.\n",
      "[2/7][599/1343]\tLoss: 0.5768  Batch   600  of  1,343.    Elapsed: 0:10:13.\n",
      "[2/7][699/1343]\tLoss: 0.9237  Batch   700  of  1,343.    Elapsed: 0:11:55.\n",
      "[2/7][799/1343]\tLoss: 0.5328  Batch   800  of  1,343.    Elapsed: 0:13:37.\n",
      "[2/7][899/1343]\tLoss: 0.5782  Batch   900  of  1,343.    Elapsed: 0:15:19.\n",
      "[2/7][999/1343]\tLoss: 0.5908  Batch 1,000  of  1,343.    Elapsed: 0:17:01.\n",
      "[2/7][1099/1343]\tLoss: 0.9132  Batch 1,100  of  1,343.    Elapsed: 0:18:43.\n",
      "[2/7][1199/1343]\tLoss: 0.3052  Batch 1,200  of  1,343.    Elapsed: 0:20:26.\n",
      "[2/7][1299/1343]\tLoss: 0.6276  Batch 1,300  of  1,343.    Elapsed: 0:22:08.\n",
      "[2/7][1342/1343]\tLoss: 0.0079\n",
      "  Average training loss: 0.65\n",
      "  Training epcoh took: 0:22:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.68\n",
      "  Validation took: 0:00:53\n",
      "\n",
      "======== Epoch 3 / 7 ========\n",
      "Training...\n",
      "[3/7][99/1343]\tLoss: 0.3065  Batch   100  of  1,343.    Elapsed: 0:01:42.\n",
      "[3/7][199/1343]\tLoss: 0.1712  Batch   200  of  1,343.    Elapsed: 0:03:24.\n",
      "[3/7][299/1343]\tLoss: 0.4482  Batch   300  of  1,343.    Elapsed: 0:05:06.\n",
      "[3/7][399/1343]\tLoss: 0.3706  Batch   400  of  1,343.    Elapsed: 0:06:48.\n",
      "[3/7][499/1343]\tLoss: 0.1504  Batch   500  of  1,343.    Elapsed: 0:08:30.\n",
      "[3/7][599/1343]\tLoss: 0.3217  Batch   600  of  1,343.    Elapsed: 0:10:12.\n",
      "[3/7][699/1343]\tLoss: 0.3902  Batch   700  of  1,343.    Elapsed: 0:11:54.\n",
      "[3/7][799/1343]\tLoss: 0.5807  Batch   800  of  1,343.    Elapsed: 0:13:36.\n",
      "[3/7][899/1343]\tLoss: 0.3221  Batch   900  of  1,343.    Elapsed: 0:15:18.\n",
      "[3/7][999/1343]\tLoss: 0.4066  Batch 1,000  of  1,343.    Elapsed: 0:17:01.\n",
      "[3/7][1099/1343]\tLoss: 0.3927  Batch 1,100  of  1,343.    Elapsed: 0:18:43.\n",
      "[3/7][1199/1343]\tLoss: 0.4156  Batch 1,200  of  1,343.    Elapsed: 0:20:24.\n",
      "[3/7][1299/1343]\tLoss: 0.3265  Batch 1,300  of  1,343.    Elapsed: 0:22:07.\n",
      "[3/7][1342/1343]\tLoss: 0.3801\n",
      "  Average training loss: 0.41\n",
      "  Training epcoh took: 0:22:50\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.69\n",
      "  Validation took: 0:00:53\n",
      "\n",
      "======== Epoch 4 / 7 ========\n",
      "Training...\n",
      "[4/7][99/1343]\tLoss: 0.2882  Batch   100  of  1,343.    Elapsed: 0:01:42.\n",
      "[4/7][199/1343]\tLoss: 0.2089  Batch   200  of  1,343.    Elapsed: 0:03:24.\n",
      "[4/7][299/1343]\tLoss: 0.3564  Batch   300  of  1,343.    Elapsed: 0:05:06.\n",
      "[4/7][399/1343]\tLoss: 0.1547  Batch   400  of  1,343.    Elapsed: 0:06:48.\n",
      "[4/7][499/1343]\tLoss: 0.2026  Batch   500  of  1,343.    Elapsed: 0:08:30.\n",
      "[4/7][599/1343]\tLoss: 0.3270  Batch   600  of  1,343.    Elapsed: 0:10:12.\n",
      "[4/7][699/1343]\tLoss: 0.2474  Batch   700  of  1,343.    Elapsed: 0:11:55.\n",
      "[4/7][799/1343]\tLoss: 0.1244  Batch   800  of  1,343.    Elapsed: 0:13:37.\n",
      "[4/7][899/1343]\tLoss: 0.1519  Batch   900  of  1,343.    Elapsed: 0:15:19.\n",
      "[4/7][999/1343]\tLoss: 0.2642  Batch 1,000  of  1,343.    Elapsed: 0:17:01.\n",
      "[4/7][1099/1343]\tLoss: 0.4044  Batch 1,100  of  1,343.    Elapsed: 0:18:43.\n",
      "[4/7][1199/1343]\tLoss: 0.4832  Batch 1,200  of  1,343.    Elapsed: 0:20:25.\n",
      "[4/7][1299/1343]\tLoss: 0.4706  Batch 1,300  of  1,343.    Elapsed: 0:22:07.\n",
      "[4/7][1342/1343]\tLoss: 0.8499\n",
      "  Average training loss: 0.29\n",
      "  Training epcoh took: 0:22:50\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.69\n",
      "  Validation took: 0:00:53\n",
      "\n",
      "======== Epoch 5 / 7 ========\n",
      "Training...\n",
      "[5/7][99/1343]\tLoss: 0.2296  Batch   100  of  1,343.    Elapsed: 0:01:42.\n",
      "[5/7][199/1343]\tLoss: 0.1604  Batch   200  of  1,343.    Elapsed: 0:03:24.\n",
      "[5/7][299/1343]\tLoss: 0.3096  Batch   300  of  1,343.    Elapsed: 0:05:06.\n",
      "[5/7][399/1343]\tLoss: 0.2445  Batch   400  of  1,343.    Elapsed: 0:06:48.\n",
      "[5/7][499/1343]\tLoss: 0.4078  Batch   500  of  1,343.    Elapsed: 0:08:30.\n",
      "[5/7][599/1343]\tLoss: 0.0506  Batch   600  of  1,343.    Elapsed: 0:10:12.\n",
      "[5/7][699/1343]\tLoss: 0.1414  Batch   700  of  1,343.    Elapsed: 0:11:54.\n",
      "[5/7][799/1343]\tLoss: 0.0948  Batch   800  of  1,343.    Elapsed: 0:13:36.\n",
      "[5/7][899/1343]\tLoss: 0.1054  Batch   900  of  1,343.    Elapsed: 0:15:18.\n",
      "[5/7][999/1343]\tLoss: 0.3066  Batch 1,000  of  1,343.    Elapsed: 0:17:00.\n",
      "[5/7][1099/1343]\tLoss: 0.1289  Batch 1,100  of  1,343.    Elapsed: 0:18:42.\n",
      "[5/7][1199/1343]\tLoss: 0.1518  Batch 1,200  of  1,343.    Elapsed: 0:20:25.\n",
      "[5/7][1299/1343]\tLoss: 0.0533  Batch 1,300  of  1,343.    Elapsed: 0:22:07.\n",
      "[5/7][1342/1343]\tLoss: 0.2833\n",
      "  Average training loss: 0.21\n",
      "  Training epcoh took: 0:22:50\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.69\n",
      "  Validation took: 0:00:53\n",
      "\n",
      "======== Epoch 6 / 7 ========\n",
      "Training...\n",
      "[6/7][99/1343]\tLoss: 0.2263  Batch   100  of  1,343.    Elapsed: 0:01:42.\n",
      "[6/7][199/1343]\tLoss: 0.0954  Batch   200  of  1,343.    Elapsed: 0:03:24.\n",
      "[6/7][299/1343]\tLoss: 0.1318  Batch   300  of  1,343.    Elapsed: 0:05:07.\n",
      "[6/7][399/1343]\tLoss: 0.0231  Batch   400  of  1,343.    Elapsed: 0:06:49.\n",
      "[6/7][499/1343]\tLoss: 0.1134  Batch   500  of  1,343.    Elapsed: 0:08:31.\n",
      "[6/7][599/1343]\tLoss: 0.0967  Batch   600  of  1,343.    Elapsed: 0:10:13.\n",
      "[6/7][699/1343]\tLoss: 0.2738  Batch   700  of  1,343.    Elapsed: 0:11:55.\n",
      "[6/7][799/1343]\tLoss: 0.1846  Batch   800  of  1,343.    Elapsed: 0:13:37.\n",
      "[6/7][899/1343]\tLoss: 0.1592  Batch   900  of  1,343.    Elapsed: 0:15:19.\n",
      "[6/7][999/1343]\tLoss: 0.1668  Batch 1,000  of  1,343.    Elapsed: 0:17:01.\n",
      "[6/7][1099/1343]\tLoss: 0.0821  Batch 1,100  of  1,343.    Elapsed: 0:18:43.\n",
      "[6/7][1199/1343]\tLoss: 0.1987  Batch 1,200  of  1,343.    Elapsed: 0:20:26.\n",
      "[6/7][1299/1343]\tLoss: 0.1188  Batch 1,300  of  1,343.    Elapsed: 0:22:08.\n",
      "[6/7][1342/1343]\tLoss: 0.0010\n",
      "  Average training loss: 0.16\n",
      "  Training epcoh took: 0:22:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.69\n",
      "  Validation took: 0:00:53\n",
      "\n",
      "======== Epoch 7 / 7 ========\n",
      "Training...\n",
      "[7/7][99/1343]\tLoss: 0.0777  Batch   100  of  1,343.    Elapsed: 0:01:42.\n",
      "[7/7][199/1343]\tLoss: 0.1230  Batch   200  of  1,343.    Elapsed: 0:03:25.\n",
      "[7/7][299/1343]\tLoss: 0.0646  Batch   300  of  1,343.    Elapsed: 0:05:07.\n",
      "[7/7][399/1343]\tLoss: 0.0487  Batch   400  of  1,343.    Elapsed: 0:06:49.\n",
      "[7/7][499/1343]\tLoss: 0.0497  Batch   500  of  1,343.    Elapsed: 0:08:31.\n",
      "[7/7][599/1343]\tLoss: 0.0542  Batch   600  of  1,343.    Elapsed: 0:10:13.\n",
      "[7/7][699/1343]\tLoss: 0.2141  Batch   700  of  1,343.    Elapsed: 0:11:55.\n",
      "[7/7][799/1343]\tLoss: 0.1122  Batch   800  of  1,343.    Elapsed: 0:13:38.\n",
      "[7/7][899/1343]\tLoss: 0.1241  Batch   900  of  1,343.    Elapsed: 0:15:20.\n",
      "[7/7][999/1343]\tLoss: 0.1015  Batch 1,000  of  1,343.    Elapsed: 0:17:02.\n",
      "[7/7][1099/1343]\tLoss: 0.0865  Batch 1,100  of  1,343.    Elapsed: 0:18:44.\n",
      "[7/7][1199/1343]\tLoss: 0.1304  Batch 1,200  of  1,343.    Elapsed: 0:20:26.\n",
      "[7/7][1299/1343]\tLoss: 0.1225  Batch 1,300  of  1,343.    Elapsed: 0:22:08.\n",
      "[7/7][1342/1343]\tLoss: 0.0117\n",
      "  Average training loss: 0.12\n",
      "  Training epcoh took: 0:22:51\n",
      "\n",
      "Running Validation...\n",
      "  Accuracy: 0.69\n",
      "  Validation took: 0:00:53\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr = 3e-5,eps = 1e-8) # ler = 5e-5\n",
    "epochs = 7\n",
    "\n",
    "total_steps = len(training_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "train(model, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred,true = get_model_predictions(model, testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6879732739420935"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([a==b for a,b in zip(pred,true)])/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.94      0.84      3000\n",
      "           1       0.41      0.38      0.39       436\n",
      "           2       0.37      0.21      0.27       209\n",
      "           3       0.11      0.01      0.01       195\n",
      "           4       0.24      0.09      0.13        99\n",
      "           5       0.10      0.07      0.08        89\n",
      "           6       0.32      0.12      0.18       130\n",
      "           7       0.48      0.21      0.29        96\n",
      "           8       0.06      0.03      0.04        33\n",
      "           9       0.25      0.06      0.09        36\n",
      "          10       0.00      0.00      0.00        56\n",
      "          11       0.00      0.00      0.00        26\n",
      "          12       0.00      0.00      0.00        22\n",
      "          13       0.33      0.04      0.07        25\n",
      "          14       0.25      0.09      0.13        11\n",
      "          15       0.00      0.00      0.00        15\n",
      "          16       0.00      0.00      0.00         4\n",
      "          17       0.00      0.00      0.00         6\n",
      "          18       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.69      4490\n",
      "   macro avg       0.19      0.12      0.13      4490\n",
      "weighted avg       0.60      0.69      0.63      4490\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/modules/Ubuntu/14.04/amd64/common/anaconda3/20190819-gpu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'ADD5_Context5_augmented.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('Context_only_augmented.pth'))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nm,true_nm = get_model_predictions(model, testing_nm_loader)\n",
    "sum([a==b for a,b in zip(pred_nm,true_nm)])/len(pred_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **testing_nm_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(true_nm, pred_nm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_nm.groupby(['prop']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(torch.cat([\n",
    "    x['targets'] for x in testing_nm_loader\n",
    "]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSP",
   "language": "python",
   "name": "dsp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
